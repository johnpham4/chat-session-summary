# Chat Assistant with Session Memory & Query Understanding

A production-ready demo of an intelligent chat assistant implementing **automatic session memory** via summarization and **query understanding** with ambiguity detection.

## ğŸ¯ Overview

This project demonstrates two core AI assistant capabilities:

1. **Session Memory via Automatic Summarization** - Automatically condenses long conversations into structured summaries when context exceeds token limits
2. **Query Understanding Pipeline** - Detects ambiguous queries, rewrites them using context, and generates clarifying questions when needed

### Key Focus Areas

- Clear pipeline design (query â†’ memory â†’ understanding â†’ response)
- Schema-first structured outputs (Pydantic validation)
- Proper context & memory management

---

## âœ¨ Features

### 1ï¸âƒ£ Session Memory via Automatic Summarization

- **Automatic trigger**: Summarization kicks in when conversation context exceeds configurable token threshold
- **Token counting**: Uses `tiktoken` for accurate token approximation
- **Structured summaries**: Generates Pydantic-validated summaries with:
  - `user_profile` (preferences, constraints)
  - `key_facts` (important information)
  - `decisions` (choices made during conversation)
  - `open_questions` (unresolved topics)
  - `todos` (action items)
- **Smart storage**: Saves summaries to PostgreSQL and soft-deletes old messages while keeping recent N messages for continuity

### 2ï¸âƒ£ Query Understanding Pipeline

Each incoming query goes through:

**Step 1: Ambiguity Detection & Rewrite**
- Detects if query is ambiguous given conversation context
- Rewrites query using session memory if possible

**Step 2: Context Augmentation**
- Combines recent N messages with session summary
- Builds enriched context for LLM

**Step 3: Clarifying Questions**
- If query remains unclear, generates 1â€“3 clarifying questions
- Returns questions instead of guessing answers

All LLM outputs are validated using **Pydantic schemas** for reliability.

---

## ğŸ—ï¸ Architecture

```
User Query
    â”‚
    â–¼
Save User Message
    â”‚
    â–¼
Load Recent Messages (limit=MAX_CONTEXT_MESSAGES)
    â”‚
    â–¼
Token Count Check (should_summarize?)
    â”‚
    â”œâ”€â”€â”€ YES â”€â”€> Summarize Session
    â”‚              â”‚
    â”‚              â”œâ”€> Generate Structured Summary
    â”‚              â”œâ”€> Store in Database
    â”‚              â””â”€> Soft-delete Old Messages
    â”‚
    â””â”€â”€â”€ NO â”€â”€â”€> Load Latest Summary (if exists)
    â”‚
    â–¼
Query Understanding (rewrite)
    â”‚
    â”œâ”€ Detect Ambiguity
    â”œâ”€ Rewrite Query (using summary + recent messages)
    â””â”€ Generate Clarifying Questions (if needed)
    â”‚
    â–¼
Ambiguity Check
    â”‚
    â”œâ”€â”€â”€ AMBIGUOUS & NO REWRITE â”€â”€> Return Clarifying Questions
    â”‚                                (early response)
    â”‚
    â””â”€â”€â”€ CLEAR or REWRITTEN â”€â”€> Context Augmentation
                                  â”‚
                                  â”œâ”€ System Prompt
                                  â”œâ”€ Summary Fields
                                  â”œâ”€ Recent Messages
                                  â””â”€ Final Query
                                  â”‚
                                  â–¼
                              LLM Generation
                                  â”‚
                                  â–¼
                              Save Assistant Message
                                  â”‚
                                  â–¼
                              Return Response
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Language** | Python 3.13+ |
| **Backend** | FastAPI |
| **UI** | Gradio |
| **LLM** | OpenAI GPT-4o-mini (via langchain-openai) |
| **Database** | PostgreSQL |
| **Async DB Driver** | asyncpg |
| **Token Counting** | tiktoken |
| **Validation** | Pydantic |
| **Logging** | loguru |
| **Package Manager** | uv |

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                    # FastAPI REST API endpoints
â”œâ”€â”€ app_ui/
â”‚   â””â”€â”€ app.py                     # Gradio web interface
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ conversation.json          # Sample exported conversation (86 messages)
â”‚   â”œâ”€â”€ questions.txt              # Sample ambiguous test queries
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ file.log               # Application runtime logs
â”œâ”€â”€ db/
â”‚   â””â”€â”€ schema.sql                 # PostgreSQL database schema
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ summarization_log.jpg      # Screenshot: auto-summarization
â”‚   â”œâ”€â”€ rewrite query.png          # Screenshot: query rewriting
â”‚   â””â”€â”€ clarrify_questions.jpg     # Screenshot: clarifying questions
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ export_conversations.py    # Export conversations to JSON
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”œâ”€â”€ settings.py            # Configuration & env variables
â”‚   â”‚   â””â”€â”€ db/postgres/
â”‚   â”‚       â”œâ”€â”€ orm.py             # Base ORM layer
â”‚   â”‚       â””â”€â”€ pool.py            # Connection pool
â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”œâ”€â”€ chat.py                # ChatSession, ChatMessage, ChatSessionSummary
â”‚   â”‚   â””â”€â”€ query.py               # Query understanding models
â”‚   â””â”€â”€ application/
â”‚       â””â”€â”€ chat/
â”‚           â”œâ”€â”€ chat.py            # ChatService (main orchestrator)
â”‚           â”œâ”€â”€ summarizer.py      # Summarization service
â”‚           â”œâ”€â”€ rewriting.py       # Query rewriting service
â”‚           â””â”€â”€ context_augment.py # Context augmentation service
â”œâ”€â”€ compose.yml                    # Docker Compose for PostgreSQL
â”œâ”€â”€ makefile                       # Convenience commands
â”œâ”€â”€ pyproject.toml                 # Python dependencies
â””â”€â”€ README.md                      # This file
```

---

## ğŸ’¾ Data Models

### Persistent Models (PostgreSQL)

**ChatSession**
```python
id: UUID
name: str
created_at: datetime
is_deleted: bool
```

**ChatMessage**
```python
id: UUID
session_id: UUID
role: Literal["system", "user", "assistant"]
content: str
created_at: datetime
is_deleted: bool
```

**ChatSessionSummary**
```python
id: UUID
session_id: UUID
user_profile: UserProfile  # {preferences: [], constraints: []}
key_facts: list[str]
decisions: list[str]
open_questions: list[str]
todos: list[str]
created_at: datetime
updated_at: datetime
```

### In-Memory Models

**QueryRewriting** - Query understanding output
**SessionContext** - Augmented context
**PreprocessResult** - Internal pipeline container

---

## ğŸŒ API Endpoints

### Sessions Management

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/sessions` | Create new chat session |
| `GET` | `/sessions?page=1&page_size=20` | List all sessions (paginated) |
| `DELETE` | `/sessions/{session_id}` | Soft-delete a session |

### Messages Management

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/sessions/{session_id}/messages?page=0&page_size=10` | Get paginated messages |
| `POST` | `/sessions/{session_id}/messages` | Send message (blocking) |
| `POST` | `/sessions/{session_id}/messages/stream` | Send message (streaming SSE) |

### CRUD Operations

- **CREATE**: `POST /sessions`, `POST /sessions/{id}/messages`
- **READ**: `GET /sessions`, `GET /sessions/{id}/messages`
- **UPDATE**: Not implemented (could add `PUT`/`PATCH` for session name)
- **DELETE**: `DELETE /sessions/{id}` (soft delete)
---

## ğŸš€ Setup

### Prerequisites

- Python 3.13+
- Docker & Docker Compose
- OpenAI API Key

### Installation

1. **Install dependencies:**
```bash
uv sync
```

2. **Configure environment:**
```bash
cp .env.example .env
# Edit .env and add:
export OPENAI_API_KEY=sk-...
```

3. **Start database:**
```bash
docker compose up -d
```

4. **Run services:**
```bash
# Terminal 1: Start backend
make endpoint

# Terminal 2: Start UI
make ui
```

The backend will be available at `http://localhost:8000` and the UI at `http://localhost:7860`.

---

## ğŸ“Š Demo Scenarios

### Flow 1: ğŸ§  Session Memory Trigger (Automatic Summarization)

**Purpose:** Demonstrate automatic summarization when conversation exceeds token threshold

**Steps:**

1. Start services:
   ```bash
   make endpoint  # Terminal 1
   make ui        # Terminal 2
   ```

2. Open UI at `http://localhost:7860`

3. Create a new chat session

4. Send **10+ message exchanges** to build context (see `data/questions.txt` for sample queries)

5. Monitor logs at `data/logs/file.log` for:
   ```
   Context exceeded threshold â†’ summarizing session
   Summarization done. Summary ID=...
   ```

6. Observe:
   - System automatically generates structured summary
   - Old messages are soft-deleted
   - Recent messages are kept for continuity

**Expected Behavior:**

- Token count exceeds `TOKEN_THRESHOLD` (default: 1000 tokens)
- `ChatSummarizeService.should_summarize()` returns `True`
- System generates Pydantic-validated summary with fields:
  - `user_profile`, `key_facts`, `decisions`, `open_questions`, `todos`
- Summary is stored in `chat_session_summary` table
- Old messages marked as `is_deleted=true` in `chat_message` table
- Subsequent queries use: `summary + recent N messages`

**Screenshot:**

![Automatic Summarization](images/summarization_log.jpg)

---

### Flow 2: ğŸ” Ambiguous Query Handling (Query Understanding)

**Purpose:** Demonstrate query rewriting and clarifying questions

**Steps:**

1. In an active chat session, send **ambiguous queries**:
   ```
   "nÃ³"       # (it - without referent)
   "Ä‘Ã³"       # (that - unclear subject)
   "á»Ÿ Ä‘Ã¢u"    # (where - missing context)
   ```

2. Check logs (`data/logs/file.log`) for:
   ```
   Query understanding: rewriting & ambiguity detection
   Query detected as ambiguous
   Rewritten query: ...
   ```

3. System behavior depends on rewrite success:

   **Scenario A: Successful Rewrite**
   - System rewrites query using context
   - Proceeds to answer with augmented context

   ![Query Rewriting](images/rewrite%20query.png)

   **Scenario B: Cannot Rewrite**
   - System detects ambiguity
   - Generates 1-3 clarifying questions
   - Returns questions instead of guessing

   ![Clarifying Questions](images/clarrify_questions.jpg)

**Expected Behavior:**

- `QueryRewritingService.rewrite()` analyzes query
- Uses `session_summary + recent_messages` as context
- Returns structured output:
  ```python
  {
    "is_ambiguous": true,
    "rewritten_query": "..." or null,
    "clarifying_questions": ["...", "..."]
  }
  ```
- If `is_ambiguous=true` and `rewritten_query=null`:
  - Returns clarifying questions to user
  - Does NOT call LLM for answer
- If `rewritten_query` exists:
  - Uses rewritten query for context augmentation
  - Proceeds to LLM generation

---

### Flow 3: ğŸ“¤ Export Conversations

Export conversation to JSON for analysis:

```bash
# Use default session from makefile
make exports

# Or specify session ID
uv run python -m scripts.export_conversations \
  --session-id session-id \
  --output my_conversation.json
```

**Sample Output:** See `data/conversation.json`

---

## ğŸ“ Test Data

### `data/conversation.json`
A complete exported conversation demonstrating:
- Multiple user/assistant exchanges (86 messages)
- Various query types (factual, creative, explanatory)
- Context building over time

### `data/questions.txt`
Sample ambiguous queries for testing:
```
Xin chÃ o, báº¡n lÃ  ai
Báº¡n ká»ƒ thá»­ má»™t cÃ¢u chuyá»‡n cÆ°á»i Ä‘i
Sao máº¥y chuyá»‡n cÆ°á»i hay dÃ¹ng Ä‘á»™ng váº­t váº­y
...
```

### `data/logs/file.log`
Application logs showing:
- Session creation and lifecycle
- Token counting: `No summarization needed of context 456 tokens`
- Summarization triggers: `Context exceeded threshold â†’ summarizing session`
- Query understanding: `Query detected as ambiguous`
- Context augmentation: `load recent messages 12 to query rewrite service`
